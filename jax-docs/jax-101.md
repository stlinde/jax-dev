# Tutorial: JAX 101
This is a tutorial developed by engineers and researchers at DeepMind.

## JAX As Accelerated NumPy
### Getting started with JAX numpy
JAX represents arrays as `DeviceArray`.
Doing this enables JAX to be run on multiple different backends - CPU, GPU and TPU - using the same code.

When a JAX function is called, the corresponding operation is dispatched to an accelerator to be computed asynchronously when possible.
The returned array is therefore not necessarily "filled in" as soon as the function returns.
Thus, if we don't require the results immediately, the computation won't block Python execution.
Therefore, unless we use `block_until_ready`, we will only time the dispatch, and not the actual computation.

### JAX first transformation: `grad`
`jax.grad` takes a numerical function written in Python and returns you a new Python function that computes the gradient of the original function.
It will only work on functions with a scalar output - it will raise an error otherwise.

By default, `jax.grad` will find the gradient with respect to the first argument.
To find the gradient with respect to a different argument (or several), you can set `argnums`.

When doing machine learning we do not need to write functions with a gigantic argument list.
This is handled by "pytrees", which is data structures of bundled arrays.

#### Value and Grad
Often you need to find both the value and the gradient of a function, e.g. if you want to log the training loss. 
`jax.value_and_grad()` returns a tuple of `(value, grad)`.

#### Auxiliary Data
In addtion to wanting to log the value, we often want to report some intermediate results obtained in computing the loss function.
`has_aux` makes this possible.
It signifies that the function returns a parir, `(out, aux)`.
It makes `jax.grad` ignore `aux`, passing it through to the user, while differentiating the function as if only `out` was returned.

#### Differences from NumPy
The most important difference is that JAX is designed to be functional, as in functional programming.
The reason behind this is that the kinds of program transformations that JAX enables are much more feasible in functional-style programs.

The important feature of functional programming to grok when working with JAX is very simple: don't write code with side-effects.

Side-effect-free code is sometimes called functionally pure, or just pure. 
Strictly the pure version is less efficient; we are creating a new array.
However, JAX computations are often compiled before being run using another program transformation, `jax.jit`.
If we don't use the old array after modifying it "in place" using indexed update operators, the compiler can recognise that it can in fact compile to an in-place modify, resulting in efficient code in the end.

#### Your first JAX training loop
To keep things simple, we'll start with a linear regression.
Our data is sampled according to `y = w_true * x + b_true + e`.
```python
import numpy as np
import matplotlib.pyplot as plt

xs = np.random.normal(size=(100,))
noise = np.random.normal(scale=0.1, size=(100,))
ys = xs * 3 - 1 + noise

plt.scatter(xs, ys)
```

Therefore our model is `y_hat(x;theta) = w * x + b`.
We will use a single array, `theta = [w, b]` to house both parameters:
```python
def model(theta, x):
    """Computes wx + b on a batch of input x."""
    return w * x + b
```
The loss function is `J(x;y;theta) = (y_hat - y)**2`.

```python
def loss_fn(theta, x, y):
    prediction = model(theta, x)
    return jnp.mean((prediction - y)**2)
```

Using gradient descent we can optimize the loss function.
At each step, we will find the gradient of the loss w.r.t. the parameters, and take a small step in the direction of steepest descent: `theta_new = theta - 0.1(grad(theta))(x, y; theta)`:
```python
def update(theta, x, y, lr=0.1):
    return theta - lr * jax.grad(loss_fn)(theta, x, y)
```

In JAX, it is common to define an `update()` function that is called at every step, taking the current parameters as input and returning the new parameters.
This is a natural consequence of JAX's functional nature, and is explained in "The Problem of State".

This function can the be JIT-compiled in its entirety for maximum efficiency.
```python
theta = jnp.array([1., 1.])

for _ in range(1000):
    theta = update(theta, xs, ys)

w, b = theta
print(f"w: {w:<.2f}, b: {b:<.2f}")
```

## Just In Time Compilation with JAX
### How JAX transforms work
JAX transforms Python functions by first converting them into an intermediate language called jaxpr.
The transformations then work on the jaxpr representation.

We can show a representation of the jaxpr of a function by using `jax.make_jaxpr`:
```python
import jax
import jax.numpy as jnp

global_list = []

def log2(x):
    global_list.append(x)
    ln_x = jnp.log(x)
    ln_2 = jnp.log(2.0)
    return ln_x / ln_2

print(jax.make_jaxpr(log2)(3.0))

# { lambda ; a:f32[]. let
#       b:f32[] = log a
#       c:f32[] = log 2.0
#       d:f32[] = div b c
#   in (d,) }
```
The *Understanding Jaxprs* section of the documentation provides more information on the meaning of the above output.

Note that jaxpr does not capture the side-effect of the function: there is nothing in it corresponding to `global_list.append(x)`.
JAX is designed to understand functionally pure code.
Impure functions can still be used, but JAX gives no guarantees about their behaviour once converted to jaxpr.
As a rule of thumb, you can expect JAX the side-effects of a JAX-transformed function to run once (during the first call), and never again, due to that way that JAX generates jaxpr, using a process called "tracing".

When tracing, JAX wraps each argument by a tracer object.
These tracers then record all JAX operations performed on them during the function call.
Then, JAX uses the tracer records to reconstruct the entire function.
The output of that reconstruction is the jaxpr.
Since the tracers do not record the Python side-effects, they do not appear in the jaxpr. 
However, the side-effects still happen during the trace itself.

The fact that the Python code runs at least once is an implementation detail, and so shouldn't be relied upon.
It can be used for debugging however.

A key thing to understand is that jaxpr captures the function as executed on the parameters given to it.
For example, if we have a conditional, jaxpr will only know about the branch we take

### JIT compiling a function
JAX provides the `jax.jit` transformation, which will JIT compile a JAX-compatible function.
```python
selu_jit = jax.jit(selu)

# Warm up - the function must be run for tracing.
selu_jit(x).block_until_ready()

%timeit selu_jit(x).block_until_ready()
#--> 10000 loops, best of 5: 150 us per loop.
```
When the function is first called it is compiled using XLA into very efficient code optimized for GPU or TPU.
Subsequent calls to the function will then use the compiled code, skipping the Python implementation.

Note that `block_until_ready()` is needed due to JAX's Asynchronous execution model.


### Why can't we just JIT everything?


## Automatic Vectorization in JAX

## Advanced Automatic Differentiation in JAX

## Pseudo Random Numbers in JAX

## Working with Pytrees

## Parallel Evaluation in JAX

## Stateful Computations in JAX

## Introduction to pjit
